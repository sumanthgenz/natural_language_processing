{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx==1.7.0\n",
    "# # !pip install tensorflow==1.15.0\n",
    "# !pip install torch==1.4.0\n",
    "# !pip install onnx_tf==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class ModelDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, utt2LabelDictPath, utt2VectorDictPath, utt2AttDictPath):\n",
    "        assert(utt2LabelDictPath != None)\n",
    "        assert(utt2VectorDictPath != None)\n",
    "        assert(utt2AttDictPath != None)\n",
    "        try :\n",
    "            with open(utt2LabelDictPath, 'rb') as handle:\n",
    "                self.utt2LabelDict = pickle.load(handle)\n",
    "            with open(utt2VectorDictPath, 'rb') as handle:\n",
    "                self.utt2VectorDict = pickle.load(handle)\n",
    "            with open(utt2AttDictPath, 'rb') as handle:\n",
    "                self.utt2AttDict = pickle.load(handle)\n",
    "        except :\n",
    "            raise Exception('Invalid path')\n",
    "            \n",
    "        self.len = len(self.utt2LabelDict.keys())\n",
    "        self.idx2Utt = {}\n",
    "        self.lab2LabIdMap = {}\n",
    "        self.Labels = []\n",
    "        \n",
    "        for idx, utt in enumerate(self.utt2LabelDict.keys()):\n",
    "            self.idx2Utt[idx] = utt\n",
    "            self.Labels.append(self.utt2LabelDict[utt])\n",
    "        for labId, lab in enumerate(set(self.Labels)):\n",
    "            self.lab2LabIdMap[lab] = labId\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def getNumClasses(self):\n",
    "        return len(Counter(self.Labels))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        utt = self.idx2Utt[idx]\n",
    "        vec = self.utt2VectorDict[utt]\n",
    "        lab = self.utt2LabelDict[utt]\n",
    "        labId = self.lab2LabIdMap[lab]\n",
    "        if utt in self.utt2AttDict:\n",
    "            att = self.utt2AttDict[utt]\n",
    "        else:\n",
    "            att = [0]*52\n",
    "        return vec, utt, labId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters BASELINE\n",
    "input_size = 512\n",
    "hidden_size = 220\n",
    "num_classes = 27\n",
    "num_epochs = 25\n",
    "batch_size = 40\n",
    "learning_rate = 0.001\n",
    "\n",
    "# from onnx_tf.backend import prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST dataset \n",
    "bw = \"bow_guse_\"\n",
    "lg = \"en_\"\n",
    "# lg = \"fr_\"\n",
    "# lg = \"de_\"\n",
    "# ty = \"sf_\"\n",
    "ty = \"it_\"\n",
    "# ty = \"hr_\"\n",
    "c = bw + lg + ty\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = DataLoader(c + 'train_uttlab.pickle', c + 'train_uttvec.pickle', c + 'train_uttatt.pickle') \n",
    "test_dataset = DataLoader(c +'test_uttlab.pickle', c + 'test_uttvec.pickle', c + 'train_uttatt.pickle') \n",
    "# print(len(Counter(list(train_dataset.lab2LabIdMap.values()))))\n",
    "# print(len(Counter(list(test_dataset.lab2LabIdMap.values()))))\n",
    "\n",
    "# comb = train_dataset.Labels + test_dataset.Labels\n",
    "# comb = list(Counter(comb))\n",
    "# newmap = {}\n",
    "# for i in range(len(comb)):\n",
    "#     newmap[comb[i]] = i\n",
    "    \n",
    "# train_dataset.lab2LabIdMap = newmap    \n",
    "# test_dataset.lab2LabIdMap = newmap    \n",
    "\n",
    "# train_dataset.lab2LabIdMap.update(test_dataset.lab2LabIdMap)\n",
    "# test_dataset.lab2LabIdMap = train_dataset.lab2LabIdMap\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "eval_train = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=4000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "eval_test = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=4000, \n",
    "                                          shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(Counter(list(train_dataset.lab2LabIdMap.values()))))\n",
    "print(len(Counter(list(test_dataset.lab2LabIdMap.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(train_dataset.utt2AttDict.values())[0:5])\n",
    "# print(list(train_dataset.utt2AttDict.keys())[0:5])\n",
    "# print(list(train_dataset.utt2LabelDict.values())[0:5])\n",
    "\n",
    "def ang(x,y):\n",
    "    nx = np.linalg.norm(x)\n",
    "    ny = np.linalg.norm(y)\n",
    "    cos = np.dot(x, y)/(nx * ny)\n",
    "    if cos > 1:\n",
    "        cos = 1\n",
    "    elif cos < -1:\n",
    "        cos = -1\n",
    "    return 1 - np.arccos(cos)/np.pi\n",
    "ind1 = 800\n",
    "ind2 = 12\n",
    "print(list(train_dataset.utt2VectorDict.keys())[ind1])\n",
    "print(list(train_dataset.utt2VectorDict.keys())[ind2])\n",
    "vec1 = list(test_dataset.utt2VectorDict.values())[ind1]\n",
    "vec2 = list(test_dataset.utt2VectorDict.values())[ind2]\n",
    "print(vec1.shape)\n",
    "ang(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = 46\n",
    "# two = 44\n",
    "# a = list(test_dataset.utt2VectorDict.values())[one]\n",
    "# b = list(test_dataset.utt2VectorDict.values())[two]\n",
    "# print(list(test_dataset.utt2VectorDict.keys())[one])\n",
    "# print(list(test_dataset.utt2VectorDict.keys())[two])\n",
    "# print(list(test_dataset.utt2LabelDict.values())[one])\n",
    "# print(list(test_dataset.utt2LabelDict.values())[two])\n",
    "# def ang(x,y):\n",
    "#     nx = np.linalg.norm(x)\n",
    "#     ny = np.linalg.norm(y)\n",
    "#     cos = np.dot(x, y)/(nx * ny)\n",
    "#     if cos > 1:\n",
    "#         cos = 1\n",
    "#     elif cos < -1:\n",
    "#         cos = -1\n",
    "#     return 1 - np.arccos(cos)/np.pi\n",
    "\n",
    "# print(ang(a, b))\n",
    "# print(list(test_dataset.utt2LabelDict.keys())[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pb(path_to_pb):\n",
    "    with tf.gfile.GFile(path_to_pb, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        return graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BASELINE\n",
    "###NUM CLASSES\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "if not os.path.exists('./ZS/'):\n",
    "    os.mkdir('./ZS/')\n",
    "\n",
    "if not os.path.exists('./ZSModels/'):\n",
    "    os.mkdir('./ZSModels/')    \n",
    "\n",
    "if not os.path.exists('./ZSGraphs/'):\n",
    "    os.mkdir('./ZSGraphs/')  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (vectors, utts, labId) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        vectors = vectors.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(vectors)\n",
    "        loss = criterion(outputs, labId)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 5 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "#             if epoch > 3.0:\n",
    "#                 title = './RNN/' + str(epoch+1) + \".\" + str(i+1) + '.onnx'\n",
    "#                 dummy_input = torch.rand(1, 1, 512)\n",
    "#                 dummy_output = model.emb(dummy_input)\n",
    "#                 torch.onnx.export(model, dummy_input, title, input_names=['test_input'], output_names=['test_output'])\n",
    "                                \n",
    "#                 graph_title = './RNNGraphs/' + str(epoch+1) + \".\" + str(i+1) + '.pb'\n",
    "#                 model_onnx = onnx.load(title)\n",
    "#                 tf_rep = prepare(model_onnx)\n",
    "#                 tf_rep.export_graph(graph_title)\n",
    "                \n",
    "#                 model_title = './RNNModels/' + str(epoch+1) + \".\" + str(i+1)\n",
    "#                 tf_graph = load_pb(graph_title)\n",
    "                \n",
    "#                 with tf.Session(graph=tf_graph) as sess:\n",
    "#                     sess.run(tf.global_variables_initializer())\n",
    "#                     ops = tf.get_default_graph().get_operations() \n",
    "#                     sess.run(tf.get_default_graph().get_operation_by_name('init'))\n",
    "\n",
    "#                     ou_tensor = tf_graph.get_tensor_by_name('test_output:0')\n",
    "#                     in_tensor = tf_graph.get_tensor_by_name('test_input:0')\n",
    "\n",
    "#                     tf.saved_model.simple_save(sess, model_title, inputs={'input': in_tensor}, outputs={'output': ou_tensor})\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(train_dataset.utt2VectorDict.keys())[0:8])\n",
    "print(list(train_dataset.utt2LabelDict.values())[0:8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BASELINE\n",
    "###NUM CLASSES\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "vector, atts, labId = [], [], []\n",
    "vec, att, lab = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, (vec, att, lab) in enumerate(eval_test):\n",
    "        outputs = model(vec.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    a = f1_score(lab, predicted, average='weighted')\n",
    "    print(\"Trained: \" + str(a))\n",
    "    \n",
    "    \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "vector, atts, labId = [], [], []\n",
    "vec, att, lab = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, (vectors, atts, labId) in enumerate(eval_train):\n",
    "        neighbor = KNeighborsClassifier(n_neighbors=1)\n",
    "        neighbor.fit(vectors.numpy(), labId.numpy())\n",
    "    for i, (vec, att, lab) in enumerate(eval_test):\n",
    "        predicted = neighbor.predict(vec.numpy())\n",
    "    b = f1_score(lab, predicted, average='weighted')\n",
    "    print(\"KNN: \" + str(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"BoW_Models/\" + c + str(a)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
