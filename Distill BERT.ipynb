{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ast\n",
    "import re\n",
    "import copy\n",
    "import math\n",
    "import openpyxl\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import csv\n",
    "import random\n",
    "import sklearn\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer, BertModel, BertTokenizer, XLMModel, XLMTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "import sentencepiece\n",
    "import ipywidgets\n",
    "\n",
    "# model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "#\n",
    "# model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request, intent = [], []  # train set\n",
    "def input(file):\n",
    "    data = json.load(open(file))['intents']\n",
    "    for each in data:\n",
    "        label = each['name']\n",
    "        for req in each['samples']:\n",
    "            r = req['utterance']\n",
    "            request.append(r)\n",
    "            intent.append(label)\n",
    "input('/Users/sumanth.gurram/Desktop/dummy_train_data')\n",
    "u2l = {}\n",
    "u2v = {}\n",
    "file = open('/Users/sumanth.gurram/Desktop/dummy_test_data')\n",
    "Lines = file.readlines()\n",
    "Lines = Lines[1:]\n",
    "it, rq = [], []\n",
    "for line in Lines:\n",
    "    split = line.strip().split('\\t')\n",
    "    s = split[2]\n",
    "    it.append(tent)\n",
    "    rq.append(split[1])\n",
    "intent, request = sklearn.utils.shuffle(intent, request)\n",
    "it, rq = sklearn.utils.shuffle(it, rq)\n",
    "\n",
    "# print(len(request))\n",
    "# print(len(rq))\n",
    "# print(Counter(intent))\n",
    "# print(Counter(it))\n",
    "# print(len(Counter(intent)))\n",
    "# print(len(Counter(it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector, atts, labId = [], [], []\n",
    "vec, att, lab = [], [], []\n",
    "vector, labId = [], [] #train set\n",
    "vec, lab = [], []      #test set\n",
    "\n",
    "def pooling(attention_mask, token_embeddings):\n",
    "    output_vectors = []\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = input_mask_expanded.sum(1)\n",
    "    sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "    output_vectors.append(sum_embeddings / sum_mask)\n",
    "    output_vector = torch.cat(output_vectors, 1)\n",
    "    return output_vector\n",
    "\n",
    "def xlm_encode(utts):\n",
    "    tokens = tokenizer.batch_encode_plus(utts, pad_to_max_length=True, max_length = 60, truncation = True)\n",
    "    tk = torch.tensor(tokens[\"input_ids\"])\n",
    "    mask = torch.tensor(tokens[\"attention_mask\"])\n",
    "    print(mask)\n",
    "    emb = model(tk)\n",
    "    print(emb[0].shape)\n",
    "    return emb[0], mask\n",
    "\n",
    "vectors, train_mask = xlm_encode(request)\n",
    "vec, test_mask = xlm_encode(rq)\n",
    "\n",
    "def ang(x,y):\n",
    "    nx = np.linalg.norm(x)\n",
    "    ny = np.linalg.norm(y)\n",
    "    cos = np.dot(x, y)/(nx * ny)\n",
    "    if cos > 1:\n",
    "        cos = 1\n",
    "    elif cos < -1:\n",
    "        cos = -1\n",
    "    return 1 - np.arccos(cos)/np.pi\n",
    "\n",
    "train_embeddings = train_mask.unsqueeze(2) * vectors\n",
    "test_embeddings = test_mask.unsqueeze(2) * vec\n",
    "train_embeddings = train_embeddings.reshape(train_embeddings.shape[0], -1).detach().numpy()\n",
    "test_embeddings = test_embeddings.reshape(test_embeddings.shape[0], -1).detach().numpy()\n",
    "pooled_train_vec = pooling(train_mask, vectors)\n",
    "pooled_test_vec = pooling(test_mask, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = layers.Input(shape=(60*768,))\n",
    "x2 = layers.Dense(1000, activation=\"relu\")(inputs)\n",
    "# x2 = layers.Dropout(0.2)(x2)\n",
    "logits = layers.Dense(100)(x2)\n",
    "outputs = layers.Softmax()(logits)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "embedding_model = keras.Model(inputs=inputs, outputs=logits)\n",
    "d = {lab : idx for idx, lab in enumerate(list(set(intent + it)))}\n",
    "train_lab = np.array([d[i] for i in intent])\n",
    "test_lab = np.array([d[i] for i in it])\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_embeddings, train_lab, batch_size=50, epochs=12, validation_data=(test_embeddings, test_lab)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
